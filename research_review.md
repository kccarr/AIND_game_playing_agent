The game of Go, like Chess before it, was considered too complex of a game to create a game playing agent or AI that eclipsed human expertise.

One of the challenges that researchers faced with Go is the breadth of the search space required to create an algorithm that consistently wins the game. It is near impossible to appropriately plan a search tree that represents all possible moves for Go. Games with a branding factor of b and depth of d have a whole range of options or nodes to explore (b^d). 

Using a combination of Monte Carlo tree search (MCTS) and  deep learning concepts the research team was able to train a neural network that won 99.8% against its competition. In addition, Google implemented what they call ‘value networks’ to evaluate their position on the board and ‘policy networks’ to determine the best possible move.

The MCTS technique was used to simulate games of Go and determine the value of every state from current state to the end of the game. Depending on the position of the tree node the MCTS can best select path based on selections that are higher correlated with optimal outcomes for the player. Over many simulations that process improves the optimal action for the next move by estimating and evaluating the children who will yield the greatest value.

The research team also utilized the value and policy network, neural networks, to better strategic decisions. Instead of using more handcrafted rules, like we did in the Isolation game project (e.g. heuristics, minimax, and alpha beta), the team trained neural network with specific outcomes in mind. For the value network, the outcome is to determine the probability a game tree node would yield victory and calculates the value of the current position as well. For the policy network, predicts the best move for the player to make. These networks were trained with inputs of initial data and a desired outcome which the user would provide feedback on.

With deep neural networks, compared with the more rules based programming paradigm, you don’t provide the rules. Instead, the rules or the optimum strategy is generated in a blackbox of sorts. The user feeds the neural net data and a desired outcome and the neural net figures out a set of rules or decision points that will lead to the desired outcome after training, feedback, and the backpropagation process is conducted to improved the performance of the neural net.

 The policy network was trained through supervised learning and feedback from expert players. This trained the network to estimate the most likely move an expert player would select in a given scenario. In addition, self-play and reinforcement was used in order to determine the move that would most likely like to victory in the long term rather than select moves that were beneficial in a smaller scope or within the next steps. Sometimes in-optimal short term moves are required in order to win the game (consider the famous “Game of the Century” match where Bobby Fischer sacrificed his queen to win the game). 

For the value network, self play was utilized as a training ground to determine the state of the board and the game for the context of winning the game instead of short-term beneficial moves. The system was able to test out strategies playing the game thousands of times.

In conclusion, The AlphaGo team was able to use a combination of AI strategies and Deep Learning techniques to achieve consistent and expert level play in a complex game with near infinite possibilities. The combination of these techniques makes readers question what else is possible? If neural networks can be used in self-play and reinforcement learning to play more games that a human can and develop a consistent game winning strategy, can we train neural networks to eventually develop neural network techniques we have not even considered? Or  determine the optimal combination of different techniques? Those in the field must imagine new ways forward with these exciting techniques.